{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9178b0f",
   "metadata": {},
   "source": [
    "Building a PDF chat bot — Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Reference: https://medium.com/@prithiviraj7r/building-a-pdf-chat-bot-retrieval-augmented-generation-rag-0bcf6060bbd6\n",
    "\n",
    "Date: Monday - 6th October 2025\n",
    "\n",
    "List of components:\n",
    "• Extract text from PDF docs\n",
    "• Chunking/Segmentation of text\n",
    "• Embedding text & Ingestion in Vectorstore\n",
    "• Conversation using LLMs (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f86792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "def get_pdf_content(pdf_path):\n",
    "\n",
    "    raw_text = \"\"\n",
    "\n",
    "    for document in documents:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        content = []\n",
    "        for page in reader.pages:\n",
    "            content.append(page.extract_text())\n",
    "    \n",
    "    return \"\\n\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2d0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = min(start + chunk_size, text_length)\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f576cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings and Vector Store\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def get_embeddings(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") # Fast and Free\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464ba1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation using LLMs (OpenAI)\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "def get_conversation_chain(vector_store):\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id = \"HuggingFaceH4/zephyrl-mini\",\n",
    "        temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm,\n",
    "        vector_store.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d728b0c",
   "metadata": {},
   "source": [
    "Testing Branches.\n",
    "\n",
    "01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
